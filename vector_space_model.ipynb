{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e38c08bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltkNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in c:\\python312\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Collecting joblib (from nltk)\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2025.9.18-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
      "     -------------------------------------- 41.5/41.5 kB 977.7 kB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm in c:\\users\\aarus\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\aarus\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.1/1.5 MB 3.0 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 0.3/1.5 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.5/1.5 MB 3.9 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 0.7/1.5 MB 4.0 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 0.7/1.5 MB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 0.8/1.5 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.1/1.5 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.5/1.5 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 3.5 MB/s eta 0:00:00\n",
      "Downloading regex-2025.9.18-cp312-cp312-win_amd64.whl (275 kB)\n",
      "   ---------------------------------------- 0.0/275.5 kB ? eta -:--:--\n",
      "   -------------------- ------------------- 143.4/275.5 kB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 275.5/275.5 kB 3.4 MB/s eta 0:00:00\n",
      "Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "   ---------------------------------------- 0.0/308.4 kB ? eta -:--:--\n",
      "   ----------------------------- ---------- 225.3/308.4 kB 6.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 308.4/308.4 kB 4.7 MB/s eta 0:00:00\n",
      "Installing collected packages: regex, joblib, nltk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to write executable - trying to use .deleteme logic\n",
      "ERROR: Could not install packages due to an OSError: [WinError 2] The system cannot find the file specified: 'c:\\\\Python312\\\\Scripts\\\\nltk.exe' -> 'c:\\\\Python312\\\\Scripts\\\\nltk.exe.deleteme'\n",
      "\n",
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\alpino.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_eng.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_rus.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\book_grammars.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dolch.zip.\n",
      "[nltk_data]    | Downloading package english_wordnet to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\english_wordnet.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\large_grammars.zip.\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker_tab.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\maxent_treebank_pos_tagger_tab.zip.\n",
      "[nltk_data]    | Downloading package mock_corpus to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\mwa_ppdb.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\paradigms.zip.\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pe08.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\perluniprops.zip.\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pl196x.zip.\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\porter_test.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\problem_reports.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pros_cons.zip.\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ptb.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data]    | Downloading package punkt_tab to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\rslp.zip.\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\rte.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\senseval.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\smultron.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\switchboard.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping help\\tagsets.zip.\n",
      "[nltk_data]    | Downloading package tagsets_json to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping help\\tagsets_json.zip.\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet.zip.\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\webtext.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2022 to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet2022.zip.\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ycoe.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk\n",
    "\n",
    "import os\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('all')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f784130",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\aarus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce71bd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Tokenize\n",
    "    text = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords and lemmatize\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = [word for word in text if word not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = [lemmatizer.lemmatize(word) for word in text]\n",
    "    \n",
    "    # Stemming\n",
    "    stemmer = nltk.stem.PorterStemmer()\n",
    "    text = [stemmer.stem(word) for word in text]\n",
    "    \n",
    "    text_with_soundex = []\n",
    "    for word in text:\n",
    "        text_with_soundex.append(word)  # the original word\n",
    "        soundex_code = soundex(word)    # the Soundex version\n",
    "        if soundex_code != word:        # avoiding duplicates\n",
    "            text_with_soundex.append(soundex_code)\n",
    "    \n",
    "    return text_with_soundex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a23d5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soundex(word):\n",
    "    if not word:\n",
    "        return \"\"\n",
    "    \n",
    "    word = word.upper()\n",
    "    \n",
    "    # first letter\n",
    "    soundex_code = word[0]\n",
    "    \n",
    "    # replacing consonants with digits\n",
    "    mapping = {\n",
    "        'B': '1', 'F': '1', 'P': '1', 'V': '1',\n",
    "        'C': '2', 'G': '2', 'J': '2', 'K': '2', 'Q': '2', 'S': '2', 'X': '2', 'Z': '2',\n",
    "        'D': '3', 'T': '3',\n",
    "        'L': '4',\n",
    "        'M': '5', 'N': '5',\n",
    "        'R': '6'\n",
    "    }\n",
    "    \n",
    "    # apply mapping\n",
    "    for char in word[1:]:\n",
    "        if char in mapping:\n",
    "            code = mapping[char]\n",
    "            # Don't add duplicate consecutive codes\n",
    "            if soundex_code[-1] != code:\n",
    "                soundex_code += code\n",
    "    \n",
    "    # removing vowels\n",
    "    vowels = 'AEIOUYHW'\n",
    "    filtered_code = soundex_code[0]  # again keeping first letter\n",
    "    for char in soundex_code[1:]:\n",
    "        if char not in vowels:\n",
    "            filtered_code += char\n",
    "    \n",
    "    # adding with zeros or truncate to 4 characters\n",
    "    filtered_code = (filtered_code + '000')[:4]\n",
    "    \n",
    "    return filtered_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a3b88a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read documents from a directory\n",
    "def read_documents(directory):\n",
    "    documents = {}\n",
    "    try:\n",
    "        for filename in os.listdir(directory):\n",
    "            with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "                tokenized = preprocess(text)\n",
    "                documents[filename] = tokenized\n",
    "    except Exception as e:\n",
    "        print('An error occurred:', e)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "946c4899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the dictionary of all words in the documents\n",
    "def create_dictionary(documents):\n",
    "    dictionary = set()\n",
    "    for document in documents.values():\n",
    "        dictionary.update(document)\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f267d384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the term frequency\n",
    "\n",
    "def term_frequency(documents):\n",
    "    tf = defaultdict(lambda: defaultdict(int))\n",
    "    for filename, tokens in documents.items():\n",
    "        for token in tokens:\n",
    "            tf[filename][token] += 1\n",
    "    return tf\n",
    "\n",
    "# Weighted term frequency\n",
    "\n",
    "def weighted_term_frequency(tf):\n",
    "    return 1 + math.log10(tf) if tf > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41e11061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_document_frequencies(posting_list):\n",
    "    # Implement document frequency calculation\n",
    "    document_frequencies = {}\n",
    "    for term, postings in posting_list.items():\n",
    "        document_frequencies[term] = len(postings)\n",
    "    return document_frequencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79d4cc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the inverse document frequency\n",
    "def postings_list(documents, unique_words):\n",
    "    # Initialize postings list as a defaultdict of lists\n",
    "    postings = defaultdict(list)\n",
    "    \n",
    "    # Compute term frequency for the documents\n",
    "    tf = term_frequency(documents)\n",
    "    \n",
    "    # Iterate over each unique word\n",
    "    for word in unique_words:\n",
    "        # Iterate over each document and its tokens\n",
    "        for filename, tokens in documents.items():\n",
    "            # Check if the word is in the document tokens\n",
    "            if word in tokens:\n",
    "                try:\n",
    "                    # Calculate weighted term frequency\n",
    "                    wt_tf = weighted_term_frequency(tf[filename][word])\n",
    "                except KeyError:\n",
    "                    # Handle case where term is not found in the document\n",
    "                    print(f\"Warning: Term '{word}' not found in document {filename}\")\n",
    "                    wt_tf = 0\n",
    "                # Append the filename and weighted term frequency to the postings list\n",
    "                postings[word].append((filename, wt_tf))\n",
    "    \n",
    "    return postings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75b553ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the document length\n",
    "\n",
    "def doc_length(tf):\n",
    "    doc_lengths = defaultdict(float)\n",
    "    for filename, terms in tf.items():\n",
    "        length = 0\n",
    "        for term, freq in terms.items():\n",
    "            length += (1 + math.log10(freq)) ** 2\n",
    "        doc_lengths[filename] = math.sqrt(length)\n",
    "    return doc_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e73fdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cosine similarity for the given document\n",
    "def cosine_similarity(query_wt, doc_wt, doc_len, doc_id):\n",
    "    similarity = {}\n",
    "\n",
    "    # Calculate dot product of query and document weights\n",
    "    dot_product = 0\n",
    "    for term in query_wt:\n",
    "        if term in doc_wt:\n",
    "            dot_product += query_wt[term] * doc_wt[term]\n",
    "    \n",
    "    # Calculate magnitude of the query vector\n",
    "    query_magnitude = 0\n",
    "    for weight in query_wt.values():\n",
    "        query_magnitude += weight ** 2\n",
    "    query_magnitude = math.sqrt(query_magnitude)\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    similarity[doc_id] = dot_product / (query_magnitude * doc_len[doc_id])\n",
    "    \n",
    "    return similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "404714f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tf(freq):\n",
    "    # Calculate term frequency using log normalization\n",
    "    return 1 + math.log10(freq) if freq > 0 else 0\n",
    "\n",
    "def calculate_idf(df, N):\n",
    "    # Calculate inverse document frequency\n",
    "    return math.log10(N / df)\n",
    "\n",
    "def rank_documents(documents, query, posting_list, document_frequencies, unique_words):\n",
    "    N = len(documents)  # Total number of documents\n",
    "    query_tokens = preprocess(query)  # Preprocess the query\n",
    "    unique_words_query = set(query_tokens)  # Unique words in the query\n",
    "    unique_words = unique_words.union(unique_words_query)  # Combine unique words from documents and query\n",
    "    query_vector = {}\n",
    "\n",
    "    # Calculate query tf-idf weights (ltc scheme)\n",
    "    for word in unique_words:\n",
    "        tf = query_tokens.count(word)  # Term frequency in the query\n",
    "        df = document_frequencies.get(word, 0)  # Document frequency of the term\n",
    "        if df > 0:\n",
    "            idf = calculate_idf(df, N)  # Inverse document frequency\n",
    "            query_vector[word] = calculate_tf(tf) * idf  # tf-idf weight for the query term\n",
    "\n",
    "    # Calculate document lengths (for cosine similarity)\n",
    "    doc_lengths = defaultdict(float)\n",
    "    for word, postings in posting_list.items():\n",
    "        for doc, log_tf in postings:\n",
    "            doc_lengths[doc] += log_tf ** 2\n",
    "    for doc in doc_lengths:\n",
    "        doc_lengths[doc] = math.sqrt(doc_lengths[doc])  # Finalize document lengths\n",
    "\n",
    "    # Calculate cosine similarities\n",
    "    similarities = {}\n",
    "    for doc_name in documents.keys():\n",
    "        doc_vector = {}\n",
    "        for word in unique_words:\n",
    "            posting = posting_list.get(word, [])\n",
    "            for doc, log_tf in posting:\n",
    "                if doc == doc_name:\n",
    "                    doc_vector[word] = log_tf  # Document vector for the term\n",
    "\n",
    "        # Compute cosine similarity\n",
    "        similarity = cosine_similarity(query_vector, doc_vector, doc_lengths, doc_name)\n",
    "        similarities.update(similarity)\n",
    "\n",
    "    # Sort documents by similarity and return all ranked documents\n",
    "    # ranked_docs = sorted(similarities.items(), key=lambda item: (-item[1], item[0]))\n",
    "    \n",
    "    def extract_doc_id(filename):\n",
    "        numbers = re.findall(r'\\d+', filename)\n",
    "        return int(numbers[0]) if numbers else 0\n",
    "\n",
    "    # Sort by similarity (descending) then by docID (ascending)\n",
    "    ranked_docs = sorted(similarities.items(), \n",
    "                        key=lambda item: (-item[1], extract_doc_id(item[0])))\n",
    "    return ranked_docs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3311d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranked document by relevance to query: \n",
      "Developing your Zomato business account and profile is a great way to boost your  restaurantâ€™s online reputation\n",
      "\n",
      "zomato.txt: 0.2171\n",
      "swiggy.txt: 0.1444\n",
      "youtube.txt: 0.0694\n",
      "messenger.txt: 0.0681\n",
      "instagram.txt: 0.0643\n",
      "paypal.txt: 0.0584\n",
      "Discord.txt: 0.0549\n",
      "reddit.txt: 0.0534\n",
      "bing.txt: 0.0508\n",
      "Amazon.txt: 0.0486\n",
      "\n",
      "\n",
      "Ranked document by relevance to query: \n",
      "Warwickshire, came from an ancient family and was the heiress to  some land \n",
      "\n",
      "shakespeare.txt: 0.1212\n",
      "levis.txt: 0.0312\n",
      "Amazon.txt: 0.0284\n",
      "skype.txt: 0.0239\n",
      "yahoo.txt: 0.0223\n",
      "google.txt: 0.0210\n",
      "flipkart.txt: 0.0204\n",
      "blackberry.txt: 0.0191\n",
      "whatsapp.txt: 0.0180\n",
      "reliance.txt: 0.0171\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m query \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m ranked_docs \u001b[38;5;241m=\u001b[39m \u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposting_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocument_frequencies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munique_words\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Print the results\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRanked document by relevance to query: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[15], line 10\u001b[0m, in \u001b[0;36msearch\u001b[1;34m(query, documents, posting_list, document_frequencies, unique_words)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearch\u001b[39m(query, documents, posting_list, document_frequencies, unique_words):\n\u001b[1;32m---> 10\u001b[0m     ranked_docs \u001b[38;5;241m=\u001b[39m \u001b[43mrank_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposting_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocument_frequencies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munique_words\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# Format the output\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     formatted_results \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[1;32mIn[13], line 43\u001b[0m, in \u001b[0;36mrank_documents\u001b[1;34m(documents, query, posting_list, document_frequencies, unique_words)\u001b[0m\n\u001b[0;32m     40\u001b[0m                 doc_vector[word] \u001b[38;5;241m=\u001b[39m log_tf  \u001b[38;5;66;03m# Document vector for the term\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;66;03m# Compute cosine similarity\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m     similarity \u001b[38;5;241m=\u001b[39m \u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_vector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc_vector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc_lengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m     similarities\u001b[38;5;241m.\u001b[39mupdate(similarity)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Sort documents by similarity and return all ranked documents\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# ranked_docs = sorted(similarities.items(), key=lambda item: (-item[1], item[0]))\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[11], line 18\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[1;34m(query_wt, doc_wt, doc_len, doc_id)\u001b[0m\n\u001b[0;32m     15\u001b[0m query_magnitude \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(query_magnitude)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Calculate cosine similarity\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m similarity[doc_id] \u001b[38;5;241m=\u001b[39m \u001b[43mdot_product\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_magnitude\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdoc_len\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdoc_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m similarity\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "def index_corpus(corpus_path):\n",
    "    documents = read_documents(corpus_path)\n",
    "    unique_words = create_dictionary(documents)\n",
    "    posting_list = postings_list(documents, unique_words)\n",
    "    document_frequencies = calculate_document_frequencies(posting_list)\n",
    "    \n",
    "    return documents, posting_list, document_frequencies, unique_words\n",
    "\n",
    "def search(query, documents, posting_list, document_frequencies, unique_words):\n",
    "    ranked_docs = rank_documents(documents, query, posting_list, document_frequencies, unique_words)\n",
    "    \n",
    "    # Format the output\n",
    "    formatted_results = []\n",
    "    for filename, score in ranked_docs[:10]:  # Get top 10 results\n",
    "        formatted_results.append(f\"{filename}: {score:.4f}\")\n",
    "    \n",
    "    return formatted_results\n",
    "\n",
    "# Example usage\n",
    "corpus_path = 'corpus'\n",
    "documents, posting_list, document_frequencies, unique_words = index_corpus(corpus_path)\n",
    "\n",
    "while True:\n",
    "    query = input(\"Enter your query(exit to end): \")\n",
    "\n",
    "    if query == 'exit':\n",
    "        break\n",
    "    \n",
    "    ranked_docs = search(query, documents, posting_list, document_frequencies, unique_words)\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Ranked document by relevance to query: \\n{query}\\n\")\n",
    "    for result in ranked_docs:\n",
    "        print(result)\n",
    "        \n",
    "    print(\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
