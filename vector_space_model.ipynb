{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38c08bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nltk\n",
    "\n",
    "import os\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('all')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f784130",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce71bd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Tokenize\n",
    "    text = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords and lemmatize\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = [word for word in text if word not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = [lemmatizer.lemmatize(word) for word in text]\n",
    "    \n",
    "    # Stemming\n",
    "    stemmer = nltk.stem.PorterStemmer()\n",
    "    text = [stemmer.stem(word) for word in text]\n",
    "    \n",
    "    text_with_soundex = []\n",
    "    for word in text:\n",
    "        text_with_soundex.append(word)  # the original word\n",
    "        soundex_code = soundex(word)    # the Soundex version\n",
    "        if soundex_code != word:        # avoiding duplicates\n",
    "            text_with_soundex.append(soundex_code)\n",
    "    \n",
    "    return text_with_soundex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a23d5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soundex(word):\n",
    "    if not word:\n",
    "        return \"\"\n",
    "    \n",
    "    word = word.upper()\n",
    "    \n",
    "    # first letter\n",
    "    soundex_code = word[0]\n",
    "    \n",
    "    # replacing consonants with digits\n",
    "    mapping = {\n",
    "        'B': '1', 'F': '1', 'P': '1', 'V': '1',\n",
    "        'C': '2', 'G': '2', 'J': '2', 'K': '2', 'Q': '2', 'S': '2', 'X': '2', 'Z': '2',\n",
    "        'D': '3', 'T': '3',\n",
    "        'L': '4',\n",
    "        'M': '5', 'N': '5',\n",
    "        'R': '6'\n",
    "    }\n",
    "    \n",
    "    # apply mapping\n",
    "    for char in word[1:]:\n",
    "        if char in mapping:\n",
    "            code = mapping[char]\n",
    "            # Don't add duplicate consecutive codes\n",
    "            if soundex_code[-1] != code:\n",
    "                soundex_code += code\n",
    "    \n",
    "    # removing vowels\n",
    "    vowels = 'AEIOUYHW'\n",
    "    filtered_code = soundex_code[0]  # again keeping first letter\n",
    "    for char in soundex_code[1:]:\n",
    "        if char not in vowels:\n",
    "            filtered_code += char\n",
    "    \n",
    "    # adding with zeros or truncate to 4 characters\n",
    "    filtered_code = (filtered_code + '000')[:4]\n",
    "    \n",
    "    return filtered_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3b88a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read documents from a directory\n",
    "def read_documents(directory):\n",
    "    documents = {}\n",
    "    try:\n",
    "        for filename in os.listdir(directory):\n",
    "            with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "                tokenized = preprocess(text)\n",
    "                documents[filename] = tokenized\n",
    "    except Exception as e:\n",
    "        print('An error occurred:', e)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946c4899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the dictionary of all words in the documents\n",
    "def create_dictionary(documents):\n",
    "    dictionary = set()\n",
    "    for document in documents.values():\n",
    "        dictionary.update(document)\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f267d384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the term frequency\n",
    "\n",
    "def term_frequency(documents):\n",
    "    tf = defaultdict(lambda: defaultdict(int))\n",
    "    for filename, tokens in documents.items():\n",
    "        for token in tokens:\n",
    "            tf[filename][token] += 1\n",
    "    return tf\n",
    "\n",
    "# Weighted term frequency\n",
    "\n",
    "def weighted_term_frequency(tf):\n",
    "    return 1 + math.log10(tf) if tf > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e11061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_document_frequencies(posting_list):\n",
    "    # Implement document frequency calculation\n",
    "    document_frequencies = {}\n",
    "    for term, postings in posting_list.items():\n",
    "        document_frequencies[term] = len(postings)\n",
    "    return document_frequencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d4cc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the inverse document frequency\n",
    "def postings_list(documents, unique_words):\n",
    "    # Initialize postings list as a defaultdict of lists\n",
    "    postings = defaultdict(list)\n",
    "    \n",
    "    # Compute term frequency for the documents\n",
    "    tf = term_frequency(documents)\n",
    "    \n",
    "    # Iterate over each unique word\n",
    "    for word in unique_words:\n",
    "        # Iterate over each document and its tokens\n",
    "        for filename, tokens in documents.items():\n",
    "            # Check if the word is in the document tokens\n",
    "            if word in tokens:\n",
    "                try:\n",
    "                    # Calculate weighted term frequency\n",
    "                    wt_tf = weighted_term_frequency(tf[filename][word])\n",
    "                except KeyError:\n",
    "                    # Handle case where term is not found in the document\n",
    "                    print(f\"Warning: Term '{word}' not found in document {filename}\")\n",
    "                    wt_tf = 0\n",
    "                # Append the filename and weighted term frequency to the postings list\n",
    "                postings[word].append((filename, wt_tf))\n",
    "    \n",
    "    return postings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b553ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the document length\n",
    "\n",
    "def doc_length(tf):\n",
    "    doc_lengths = defaultdict(float)\n",
    "    for filename, terms in tf.items():\n",
    "        length = 0\n",
    "        for term, freq in terms.items():\n",
    "            length += (1 + math.log10(freq)) ** 2\n",
    "        doc_lengths[filename] = math.sqrt(length)\n",
    "    return doc_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e73fdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cosine similarity for the given document\n",
    "def cosine_similarity(query_wt, doc_wt, doc_len, doc_id):\n",
    "    similarity = {}\n",
    "\n",
    "    # Calculate dot product of query and document weights\n",
    "    dot_product = 0\n",
    "    for term in query_wt:\n",
    "        if term in doc_wt:\n",
    "            dot_product += query_wt[term] * doc_wt[term]\n",
    "    \n",
    "    # Calculate magnitude of the query vector\n",
    "    query_magnitude = 0\n",
    "    for weight in query_wt.values():\n",
    "        query_magnitude += weight ** 2\n",
    "    query_magnitude = math.sqrt(query_magnitude)\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    similarity[doc_id] = dot_product / (query_magnitude * doc_len[doc_id])\n",
    "    \n",
    "    return similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404714f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tf(freq):\n",
    "    # Calculate term frequency using log normalization\n",
    "    return 1 + math.log10(freq) if freq > 0 else 0\n",
    "\n",
    "def calculate_idf(df, N):\n",
    "    # Calculate inverse document frequency\n",
    "    return math.log10(N / df)\n",
    "\n",
    "def rank_documents(documents, query, posting_list, document_frequencies, unique_words):\n",
    "    N = len(documents)  # Total number of documents\n",
    "    query_tokens = preprocess(query)  # Preprocess the query\n",
    "    unique_words_query = set(query_tokens)  # Unique words in the query\n",
    "    unique_words = unique_words.union(unique_words_query)  # Combine unique words from documents and query\n",
    "    query_vector = {}\n",
    "\n",
    "    # Calculate query tf-idf weights (ltc scheme)\n",
    "    for word in unique_words:\n",
    "        tf = query_tokens.count(word)  # Term frequency in the query\n",
    "        df = document_frequencies.get(word, 0)  # Document frequency of the term\n",
    "        if df > 0:\n",
    "            idf = calculate_idf(df, N)  # Inverse document frequency\n",
    "            query_vector[word] = calculate_tf(tf) * idf  # tf-idf weight for the query term\n",
    "\n",
    "    # Calculate document lengths (for cosine similarity)\n",
    "    doc_lengths = defaultdict(float)\n",
    "    for word, postings in posting_list.items():\n",
    "        for doc, log_tf in postings:\n",
    "            doc_lengths[doc] += log_tf ** 2\n",
    "    for doc in doc_lengths:\n",
    "        doc_lengths[doc] = math.sqrt(doc_lengths[doc])  # Finalize document lengths\n",
    "\n",
    "    # Calculate cosine similarities\n",
    "    similarities = {}\n",
    "    for doc_name in documents.keys():\n",
    "        doc_vector = {}\n",
    "        for word in unique_words:\n",
    "            posting = posting_list.get(word, [])\n",
    "            for doc, log_tf in posting:\n",
    "                if doc == doc_name:\n",
    "                    doc_vector[word] = log_tf  # Document vector for the term\n",
    "\n",
    "        # Compute cosine similarity\n",
    "        similarity = cosine_similarity(query_vector, doc_vector, doc_lengths, doc_name)\n",
    "        similarities.update(similarity)\n",
    "\n",
    "    # Sort documents by similarity and return all ranked documents\n",
    "    # ranked_docs = sorted(similarities.items(), key=lambda item: (-item[1], item[0]))\n",
    "    \n",
    "    def extract_doc_id(filename):\n",
    "    numbers = re.findall(r'\\d+', filename)\n",
    "    return int(numbers[0]) if numbers else 0\n",
    "\n",
    "    # Sort by similarity (descending) then by docID (ascending)\n",
    "    ranked_docs = sorted(similarities.items(), \n",
    "                        key=lambda item: (-item[1], extract_doc_id(item[0])))\n",
    "    return ranked_docs\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
